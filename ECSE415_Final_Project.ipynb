{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECSE415_Final_Project2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o91lCmRsPHeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bab33f5-36dc-4af9-88a5-367d81dcd5f2"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from numpy import random\n",
        "from sklearn.svm import SVC\n",
        "import csv \n",
        "from shapely.geometry import Point, Polygon\n",
        "from skimage import feature\n",
        "import os\n",
        "import cvxopt\n",
        "import cvxopt.solvers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# path = '/content/drive/MyDrive/Colab_Notebooks/ECSE415_Final_Project/' # Kun's path, comment out when needed\n",
        "path = '/content/drive/MyDrive/Colab_Notebooks/ECSE415_Final_Project/' # Jay's path\n",
        "# path = '/content/drive/My Drive/ECSE_415/Project/' # Ben's path\n",
        "# path = './' # Kamy's path"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU4Ag07uPJPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "tags": [],
        "outputId": "092eda69-a474-4026-8f46-59eb6dbed653"
      },
      "source": [
        "# Inspired from https://medium.com/@luanaebio/detecting-people-with-yolo-and-opencv-5c1f9bc6a810\n",
        "\n",
        "images = []\n",
        "path_frames = path + 'frames/'\n",
        "detected_frames = []\n",
        "positive_patches = []\n",
        "negative_patches = []\n",
        "boxes_positive_patches_array = []\n",
        "\n",
        "# load images from frames/\n",
        "for count,image_path in enumerate(os.listdir(path_frames)):\n",
        "  if count > 30:\n",
        "    break\n",
        "  input_path = os.path.join(path_frames, image_path)\n",
        "  image = plt.imread(input_path)\n",
        "  images.append(image)\n",
        "\n",
        "Width = image.shape[1]\n",
        "Height = image.shape[0]\n",
        "\n",
        "# load class names\n",
        "classes = None\n",
        "with open(path + 'coco.names', 'r') as f:\n",
        "  classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# read pre-trained model and config file\n",
        "net = cv2.dnn.readNet(path + 'yolov3.weights', path + 'cfg/yolov3.cfg')\n",
        "\n",
        "for image in images:\n",
        "  image_patches = image.copy()\n",
        "\n",
        "  # create input blob \n",
        "  # set input blob for the network\n",
        "  # blob = cv2.dnn.blobFromImage(image, scalefactor=?, size=?, mean substraction value=?, swapRB=?)\n",
        "  blob = cv2.dnn.blobFromImage(image, 0.00392, (416,416), (0,0,0), True, crop=False)\n",
        "  net.setInput(blob)\n",
        "\n",
        "  # run inference through the network\n",
        "  layer_names = net.getLayerNames()\n",
        "  output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "  # gather predictions from output layers\n",
        "  outs = net.forward(output_layers)\n",
        "\n",
        "  # initiatialization\n",
        "  class_ids = []\n",
        "  confidences = []\n",
        "  boxes = []\n",
        "  conf_threshold = 0.5\n",
        "  nms_threshold = 0.4\n",
        "  boxes_positive_patches = []\n",
        "\n",
        "  # for each detetion from each output layer get the confidence, class id, bounding box params and ignore weak detections (confidence < 0.5) \n",
        "  for out in outs:\n",
        "    for detection in out:\n",
        "      scores = detection[5:] # from 0-4, is matched box coordinates/dimension info, from 5 onwards is an array of confidence scores towards each different class in classes\n",
        "      class_id = np.argmax(scores) # return the index of max confidence\n",
        "      confidence = scores[class_id] # get the confidence score\n",
        "      if confidence > conf_threshold:\n",
        "        center_x = int(detection[0] * Width)\n",
        "        center_y = int(detection[1] * Height)\n",
        "        w = int(detection[2] * Width)\n",
        "        h = int(detection[3] * Height)\n",
        "        x = center_x - w / 2\n",
        "        y = center_y - h / 2\n",
        "\n",
        "        #####################################\n",
        "        #                                   #\n",
        "        #  moved \"extract positive patches\" #\n",
        "        #  part from here to post NMS to    #\n",
        "        #  further remove duplicated ones   #\n",
        "        #                                   #\n",
        "        #####################################\n",
        "\n",
        "        # store captured class_id number\n",
        "        class_ids.append(class_id)\n",
        "        # store the confidence towards the above class_id\n",
        "        confidences.append(float(confidence))\n",
        "        # store the captured boxes\n",
        "        boxes.append([x, y, w, h])\n",
        "        # Would need to generate negative patch here and append it to negative_patches array\n",
        "\n",
        "  # apply non-max suppression: extract the highest confidence box index among all partially overlapped boxes\n",
        "  indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold )\n",
        "\n",
        "  #print(\"Positive patches of this image:\")\n",
        "\n",
        "  #check if is people detection, if so, draw boxes in the original image, and extract positive patches\n",
        "  for i in indices:\n",
        "    i = i[0] # i was a 1x1 array, make it a scaler for indexing into boxes\n",
        "    box = boxes[i]\n",
        "    if class_ids[i]==0:\n",
        "      label = str(classes[class_id])\n",
        "\n",
        "      ################ extract positive patches starts ##############\n",
        "\n",
        "      x = box[0]\n",
        "      y = box[1]\n",
        "      w = box[2]\n",
        "      h = box[3]\n",
        "      if w > 100 or w < 1 or h > 200 or h < 1:\n",
        "        continue\n",
        "      positive_patch = np.squeeze(image_patches)\n",
        "      positive_patch = positive_patch[int(y):int(y+h),int(x):int(x+w)]\n",
        "      positive_patches.append(positive_patch)\n",
        "\n",
        "      # extract positive patch box in an array \n",
        "      boxes_positive_patches.append(box)\n",
        "\n",
        "      ############ show positive patches ############\n",
        "\n",
        "      # plt.imshow(positive_patch)\n",
        "      # plt.show()\n",
        "\n",
        "      ############### extract positive patches end #################\n",
        "      \n",
        "      # draw selected boxes in the original image\n",
        "      cv2.rectangle(image, (round(box[0]),round(box[1])), (round(box[0]+box[2]),round(box[1]+box[3])), (255, 0, 0), 2)\n",
        "      cv2.putText(image, label, (round(box[0])-10,round(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "  ############## extract negative patches start ###############\n",
        "\n",
        "  #print(\"Negative patches of this image\")\n",
        "\n",
        "  # foreach positie patch, create a negative patch at a random location that does not intersect with any of the positive patches\n",
        "  for boxA in boxes_positive_patches:\n",
        "\n",
        "    # extract positive patch dimension\n",
        "    wA = boxA[2]\n",
        "    hA = boxA[3]\n",
        "\n",
        "    max_xA1 = Width - wA\n",
        "    max_yA1 = Height - hA\n",
        "\n",
        "    # initialize the random image status to dirty until loop through all positive patches to confirm no intersactions\n",
        "    n_patch_status = 'dirty'\n",
        "\n",
        "    # generate negative patch candidates if patch status is 'dirty'\n",
        "    while n_patch_status == 'dirty':\n",
        "      # generate a negative patch candidate with the same dimension as the positive patch but located randomly elsewhere in the image\n",
        "      xA1 = np.random.randint(0,max_xA1)\n",
        "      yA1 = np.random.randint(0,max_yA1)\n",
        "      xA2 = xA1 + wA\n",
        "      yA2 = yA1 + hA\n",
        "      polyA = Polygon([(xA1,yA1),(xA2,yA1),(xA2,yA2),(xA1,yA2),(xA1,yA1)])\n",
        "      # p1 = Point(xA1,yA1)\n",
        "      # p2 = Point(xA1,yA2)\n",
        "      # p3 = Point(xA2,yA1)\n",
        "      # p4 = Point(xA2,yA2)\n",
        "      # print (f'w range: {wA}, h range: {hA}, xA1 = {xA1}, yA1 = {yA1}')\n",
        "      # check the negative patch candidate against each of the positive patches\n",
        "      for boxB in boxes_positive_patches:\n",
        "        xB1 = boxB[0]\n",
        "        yB1 = boxB[1]\n",
        "        wB = boxB[2]\n",
        "        hB = boxB[3]\n",
        "        xB2 = xB1 + wB\n",
        "        yB2 = yB1 + hB\n",
        "        polyB = Polygon([(xB1,yB1),(xB2,yB1),(xB2,yB2),(xB1,yB2),(xB1,yB1)])\n",
        "        #if ((xB1<=xA1<=xB2 and yB1<=yA1<=yB2) or (xB1<=xA1<=xB2 and yB1<=yA1<=yB2) or (xB1<=xA1<=xB2 and yB1<=yA1<=yB2))\n",
        "        # if any corner of the negative patch candidate falls within the positive patch, disregard and generate a new one\n",
        "        #if p1.within(polyB) == 'True' or p2.within(polyB) == 'True' or p3.within(polyB) == 'True' or p4.within(polyB) == 'True':\n",
        "        if Polygon(polyA).intersects(Polygon(polyB)):\n",
        "          n_patch_status = 'dirty'\n",
        "          break\n",
        "        # else temporarily set the negative patch candidate as clean\n",
        "        else:\n",
        "          n_patch_status = 'clean'\n",
        "      # after a particular negative patch candidate has been confirmed not intersecting with any of the positive patches, extract it.\n",
        "      if n_patch_status == 'clean':\n",
        "        negative_patch = np.squeeze(image_patches)\n",
        "        negative_patch = negative_patch[int(yA1):int(yA2),int(xA1):int(xA2)]\n",
        "        negative_patches.append(negative_patch)\n",
        "\n",
        "        ########### show negative patches ###########\n",
        "\n",
        "        # plt.imshow(negative_patch)\n",
        "        # plt.show()\n",
        "\n",
        "  ############## extract negative patches end ###############\n",
        "\n",
        "  detected_frames.append(image)\n",
        "  boxes_positive_patches_array.append(boxes_positive_patches)\n",
        "\n",
        "  ########### show image ########### \n",
        "\n",
        "  #show image\n",
        "  # print(\"This image:\")\n",
        "  # plt.imshow(image)\n",
        "  # plt.show()\n",
        "\n",
        "print('Detected people in ' + str(len(detected_frames)) + ' frames which are stored in the detected_images array')\n",
        "print('Extracted ' + str(len(positive_patches)) + ' positive patches which are stored in the positive_patches array')\n",
        "print('Extracted ' + str(len(negative_patches)) + ' negative patches which are stored in the negative_patches array')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected people in 31 frames which are stored in the detected_images array\n",
            "Extracted 610 positive patches which are stored in the positive_patches array\n",
            "Extracted 610 negative patches which are stored in the negative_patches array\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "id": "iHnt5lFWxTmi",
        "outputId": "4687822d-b160-4a78-c871-6eee74438be1"
      },
      "source": [
        "## Function to compute Local Binary Pattern (LBP) \n",
        "\n",
        "# source: https://www.pyimagesearch.com/2015/12/07/local-binary-patterns-with-python-opencv/\n",
        "def LBP(images, numPoints, radius, eps=1e-7):\n",
        "    # compute the Local Binary Pattern representation\n",
        "    # of the image, and then use the LBP representation\n",
        "    # to build the histogram of patterns\n",
        "    features = []\n",
        "    for count,image in enumerate(images):\n",
        "        if image.shape[1] != 0:\n",
        "          gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "          lbp = feature.local_binary_pattern(gray, numPoints,\n",
        "              radius, method=\"uniform\")\n",
        "          (hist, _) = np.histogram(lbp.ravel(),\n",
        "              bins=np.arange(0, numPoints + 3),\n",
        "              range=(0, numPoints + 2))\n",
        "\n",
        "          # normalize the histogram\n",
        "          hist = hist.astype(\"float\")\n",
        "          hist /= (hist.sum() + eps)\n",
        "\n",
        "          # return the histogram of Local Binary Patterns\n",
        "          features.append(hist)\n",
        "    return features\n",
        "negative_patches_LBP = LBP(images=np.array(negative_patches), numPoints=24, radius=8)\n",
        "positive_patches_LBP = LBP(images=np.array(positive_patches), numPoints=24, radius=8)\n",
        "\n",
        "plt.imshow(positive_patches_LBP[0].reshape(1, -1))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdtkdcW1s23R",
        "tags": [],
        "outputId": "5b017c99-56f6-41c3-9868-a7d2f65ae5fa"
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "\n",
        "for p in positive_patches_LBP:\n",
        "  x.append(p)\n",
        "  y.append(1.)\n",
        "\n",
        "for n in negative_patches_LBP:\n",
        "  x.append(n)\n",
        "  y.append(-1.)\n",
        "\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(913, 26) (913,)\n",
            "(305, 26) (305,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFrcTyTX88dO"
      },
      "source": [
        "class SVM():\n",
        "  def __init__(self):\n",
        "    self.SV = None\n",
        "    self.SL = None\n",
        "    self.support = None\n",
        "    self.alphas = None\n",
        "    self.intercept = None\n",
        "    self.indices = None\n",
        "    self.weights = None\n",
        "  \n",
        "  # Non-linear function used for our SVM\n",
        "  def radial_basis_function(self, x, y):\n",
        "    gamma = 10.\n",
        "    x_y = np.subtract(x, y)\n",
        "    x_y_trans = x_y.T\n",
        "    return np.exp(-(gamma * np.dot(x_y_trans, x_y)))\n",
        "\n",
        "  # Trains our SVM\n",
        "  def fit(self, data, labels):\n",
        "    d_count, _ = data.shape\n",
        "    \n",
        "    # Create initial transforms\n",
        "    k = np.zeros([d_count,d_count])\n",
        "    for i in range(d_count):\n",
        "      for j in range(d_count):\n",
        "        k[i,j] = self.radial_basis_function(data[i],data[j])\n",
        "\n",
        "    # Defining required variables for CVXOPT used for convex optimization purposes\n",
        "    P = cvxopt.matrix(np.outer(labels,labels)*k)\n",
        "    q = cvxopt.matrix(np.ones(d_count)*-1)\n",
        "    G = cvxopt.matrix(np.diag(np.ones(d_count) * -1))\n",
        "    h = cvxopt.matrix(np.zeros(d_count))\n",
        "    A = cvxopt.matrix(labels,(1,d_count))\n",
        "    b = cvxopt.matrix(0.0)\n",
        "\n",
        "    # Solving for x\n",
        "    target = 'x'\n",
        "    res = cvxopt.solvers.qp(P, q, G, h, A, b)[target]\n",
        "    res_flat = np.ravel(res)\n",
        "    select = res_flat > 1e-5\n",
        "\n",
        "    # Setting the different class variables from the results we got from CVXOPT\n",
        "    self.intercept = 0\n",
        "    self.SV = data[select]\n",
        "    self.SL = labels[select]\n",
        "    self.alphas = res_flat[select]\n",
        "    self.indices = np.arange(d_count)[select]\n",
        "    self.support = np.sum(select)\n",
        "\n",
        "    # Optimize\n",
        "    count = self.alphas.shape[0]\n",
        "    for index in range(count):\n",
        "      self.intercept += self.SL[index]\n",
        "      transform = k[self.indices[index], select]\n",
        "      curr = transform * self.SL * self.alphas\n",
        "      self.intercept -= np.sum(curr)\n",
        "    self.intercept /= count\n",
        "\n",
        "  # Predicts using our trained SVM\n",
        "  def predict(self, data):\n",
        "    size = data.shape[0]\n",
        "    res = np.zeros(size)\n",
        "    for i in range(size):\n",
        "      s = 0\n",
        "      # Assuming alphas SL and SV have same size\n",
        "      for j in range(len(self.SV)):\n",
        "        a = self.alphas[j]\n",
        "        l = self.SL[j]\n",
        "        sv = self.SV[j]\n",
        "        s += a * l * self.radial_basis_function(data[i], sv)\n",
        "      res[i] = s\n",
        "    res += self.intercept\n",
        "    return np.where(res>0,1,-1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY1ekm4-9B6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7681f2a-dea2-4182-9c3a-aaeebdd2d775"
      },
      "source": [
        "model = SVM()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "svclassifier = SVC(kernel='poly')\n",
        "svclassifier.fit(X_train, y_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -5.3263e+02 -1.7190e+03  4e+03  4e+01  3e+00\n",
            " 1: -1.6378e+03 -3.6143e+03  3e+03  3e+01  2e+00\n",
            " 2: -4.5219e+03 -7.2165e+03  3e+03  2e+01  1e+00\n",
            " 3: -7.5576e+03 -1.1062e+04  4e+03  2e+01  1e+00\n",
            " 4: -1.8631e+04 -2.3603e+04  5e+03  2e+01  1e+00\n",
            " 5: -5.0211e+04 -5.9019e+04  9e+03  2e+01  1e+00\n",
            " 6: -1.4934e+05 -1.6812e+05  2e+04  2e+01  1e+00\n",
            " 7: -2.9439e+05 -3.2625e+05  3e+04  2e+01  1e+00\n",
            " 8: -4.9218e+05 -5.4197e+05  5e+04  2e+01  1e+00\n",
            " 9: -9.6647e+05 -1.0624e+06  1e+05  2e+01  1e+00\n",
            "10: -1.9791e+06 -2.1944e+06  2e+05  2e+01  1e+00\n",
            "11: -3.9563e+06 -4.5010e+06  5e+05  1e+01  1e+00\n",
            "12: -7.4694e+06 -8.8747e+06  1e+06  1e+01  9e-01\n",
            "13: -1.4007e+07 -1.7636e+07  4e+06  9e+00  6e-01\n",
            "14: -1.7361e+07 -1.8983e+07  2e+06  1e+00  8e-02\n",
            "15: -1.7476e+07 -1.7588e+07  1e+05  7e-02  5e-03\n",
            "16: -1.7486e+07 -1.7489e+07  4e+03  2e-03  1e-04\n",
            "17: -1.7486e+07 -1.7486e+07  9e+01  5e-05  4e-06\n",
            "18: -1.7486e+07 -1.7486e+07  1e+00  6e-07  4e-08\n",
            "19: -1.7486e+07 -1.7486e+07  1e-02  6e-09  2e-09\n",
            "Optimal solution found.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4JzQONh8-Sf",
        "tags": [],
        "outputId": "cd2c4b46-2a18-4aa8-bb19-0ce11a71e86e"
      },
      "source": [
        "prediction = model.predict(X_test)\n",
        "prediction2 = svclassifier.predict(X_test)\n",
        "\n",
        "correct1 = 0\n",
        "correct2 = 0\n",
        "for i in range(len(list(y_test))):\n",
        "  if y_test[i] == prediction[i]:\n",
        "    correct1 += 1\n",
        "  if y_test[i] == prediction2[i]:\n",
        "    correct2 += 1\n",
        "\n",
        "\n",
        "print(correct1/len(list(y_test)))\n",
        "print(correct2/len(list(y_test)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7475409836065574\n",
            "0.7573770491803279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "BGAA7uGB_aAi",
        "outputId": "d05f743a-b2e2-475a-c1b8-f90972753a7a"
      },
      "source": [
        "predictions_per_image = []\n",
        "svm_boxes_array = []\n",
        "for count,image_path in enumerate(os.listdir(path_frames)):\n",
        "    if count >= len(detected_frames):\n",
        "        break\n",
        "    #sliding window along one frame\n",
        "    input_path = os.path.join(path_frames, image_path)\n",
        "    image = plt.imread(input_path)\n",
        "    # print(image.shape)\n",
        "    # plt.figure(figsize=(10, 10))\n",
        "    # plt.imshow(image)\n",
        "    # plt.show()\n",
        "    windows = []\n",
        "    our_boxes = []\n",
        "    #sliding window, window size grows as we go down\n",
        "    for a in range(0,621,20):\n",
        "        for b in range(0,101,50):\n",
        "            temp_image = image[b:b+50,a:a+20]\n",
        "            windows.append(temp_image)\n",
        "            our_boxes.append([b, a, 50, 20])\n",
        "    for a in range(0,601,40):\n",
        "        for b in range(100,301,50):\n",
        "            temp_image = image[b:b+90,a:a+40]\n",
        "            windows.append(temp_image)\n",
        "            our_boxes.append([b, a, 90, 40])\n",
        "    for a in range(0,591,10):\n",
        "        for b in range(300,381,80):\n",
        "            temp_image = image[b:b+100,a:a+50]\n",
        "            windows.append(temp_image)\n",
        "            our_boxes.append([b, a, 100, 50])\n",
        "\n",
        "\n",
        "    # plt.figure(figsize=(5, 5))\n",
        "    # plt.imshow(windows[-1])\n",
        "    # plt.show()\n",
        "    # print('Detected ' + str(len(windows)) + ' windows.')\n",
        "\n",
        "    np_windows = np.array(windows)\n",
        "    # print(np_windows.shape)\n",
        "    # print(\"getting features\")\n",
        "    test_windows = LBP(images=np_windows, numPoints=24, radius=8)\n",
        "    # print(\"making predictions\")\n",
        "    predictions = model.predict(np.array(test_windows))\n",
        "    #put in seperate block so you don't rerun the last block\n",
        "    pos_windows = []\n",
        "    our_pos_boxes = []\n",
        "    for i in range(len(predictions)):\n",
        "        if (predictions[i] == 1):\n",
        "            pos_windows.append(np_windows[i])\n",
        "            our_pos_boxes.append(our_boxes[i])\n",
        "    np_pos_windows = np.array(pos_windows)\n",
        "\n",
        "    # print(np_pos_windows.shape)\n",
        "    # for im in np_pos_windows:\n",
        "    #   plt.imshow(im)\n",
        "    #   plt.show()\n",
        "\n",
        "\n",
        "    delete = []\n",
        "    for f in range(np_pos_windows.shape[0]):\n",
        "        delete.append(1)\n",
        "    for a in range(np_pos_windows.shape[0]):\n",
        "        if (delete[a] == 1):\n",
        "            hsv1 = cv2.cvtColor(np_pos_windows[a], cv2.COLOR_BGR2HSV)\n",
        "            hist1 = cv2.calcHist([hsv1], [0,1], None, [180,256], [0,180,0,256])\n",
        "            cv2.normalize(hist1, hist1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
        "            for b in range(np_pos_windows.shape[0]):\n",
        "                if (delete[b]==1):\n",
        "                    hsv2 = cv2.cvtColor(np_pos_windows[b], cv2.COLOR_BGR2HSV)\n",
        "                    hist2 = cv2.calcHist([hsv2], [0,1], None, [180,256], [0,180,0,256])\n",
        "                    cv2.normalize(hist2, hist2, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
        "                    difference = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)\n",
        "                    #print(difference)\n",
        "                    if (difference > 0.30 and b != a):\n",
        "                        delete[b] = 0\n",
        "\n",
        "    # print(delete)\n",
        "\n",
        "    im = image.copy()\n",
        "    people_count = 0\n",
        "    positive_boxes = []\n",
        "    for i in range(len(np_pos_windows)):\n",
        "        if (delete[i] != 0):\n",
        "            # draw selected boxes in the original image\n",
        "            cv2.rectangle(im, (round(our_pos_boxes[i][1]),round(our_pos_boxes[i][0])), (round(our_pos_boxes[i][1]+our_pos_boxes[i][3]),round(our_pos_boxes[i][0]+our_pos_boxes[i][2])), (255, 0, 0), 2)\n",
        "            people_count += 1\n",
        "            positive_boxes.append(our_pos_boxes[i])\n",
        "\n",
        "    # only keep as many boxes as yolo frames for IoU computation\n",
        "    if count < len(detected_frames):\n",
        "        svm_boxes_array.append(positive_boxes)\n",
        "        print(count)\n",
        "                \n",
        "        \n",
        "    # plt.figure(figsize=(15, 15))\n",
        "    # plt.imshow(im)\n",
        "    # plt.show()\n",
        "    print(\"image: \" + str(image_path) + \" people count: \" + str(people_count)) \n",
        "    predictions_per_image.append([count+1, people_count])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "image: seq_000001.jpg people count: 26\n",
            "1\n",
            "image: seq_000002.jpg people count: 21\n",
            "2\n",
            "image: seq_000003.jpg people count: 26\n",
            "3\n",
            "image: seq_000004.jpg people count: 25\n",
            "4\n",
            "image: seq_000005.jpg people count: 27\n",
            "5\n",
            "image: seq_000006.jpg people count: 27\n",
            "6\n",
            "image: seq_000007.jpg people count: 37\n",
            "7\n",
            "image: seq_000008.jpg people count: 29\n",
            "8\n",
            "image: seq_000009.jpg people count: 24\n",
            "9\n",
            "image: seq_000010.jpg people count: 27\n",
            "10\n",
            "image: seq_000011.jpg people count: 29\n",
            "11\n",
            "image: seq_000012.jpg people count: 29\n",
            "12\n",
            "image: seq_000013.jpg people count: 37\n",
            "13\n",
            "image: seq_000014.jpg people count: 29\n",
            "14\n",
            "image: seq_000015.jpg people count: 36\n",
            "15\n",
            "image: seq_000016.jpg people count: 24\n",
            "16\n",
            "image: seq_000017.jpg people count: 23\n",
            "17\n",
            "image: seq_000018.jpg people count: 21\n",
            "18\n",
            "image: seq_000019.jpg people count: 33\n",
            "19\n",
            "image: seq_000020.jpg people count: 33\n",
            "20\n",
            "image: seq_000021.jpg people count: 30\n",
            "21\n",
            "image: seq_000022.jpg people count: 29\n",
            "22\n",
            "image: seq_000023.jpg people count: 31\n",
            "23\n",
            "image: seq_000024.jpg people count: 29\n",
            "24\n",
            "image: seq_000025.jpg people count: 23\n",
            "25\n",
            "image: seq_000026.jpg people count: 30\n",
            "26\n",
            "image: seq_000027.jpg people count: 34\n",
            "27\n",
            "image: seq_000028.jpg people count: 30\n",
            "28\n",
            "image: seq_000029.jpg people count: 21\n",
            "29\n",
            "image: seq_000030.jpg people count: 28\n",
            "30\n",
            "image: seq_000031.jpg people count: 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "tM1wvi6c_aAj",
        "outputId": "217fba6c-f1f0-465c-cf04-2cab9dd24801"
      },
      "source": [
        "\n",
        "# Making masks from bounding boxes for each frame\n",
        "def make_mask(frame, bounding_boxes, yolo=True):\n",
        "    # \ta- initialise mask (numpy array) of frame size to 0s.\n",
        "    mask = np.zeros((frame.shape[0], frame.shape[1]))\n",
        "    # \tb- for each bounding box in frame, set \n",
        "    for box in bounding_boxes:\n",
        "        x = int(box[0])\n",
        "        y = int(box[1])\n",
        "        w = int(box[2])\n",
        "        h = int(box[3])\n",
        "        ## TODO: FIX THE BOX X,Y Conventions to avoid the code below\n",
        "        if yolo:\n",
        "            mask[y: y+h, x: x+w] = 255\n",
        "        else: \n",
        "            mask[x: x+w, y: y+h] = 255\n",
        "\n",
        "    \n",
        "    return mask\n",
        "\n",
        "# Calculating IoU\n",
        "intersections = []\n",
        "unions = []\n",
        "for count, frame in enumerate(detected_frames):\n",
        "    yolo_mask = make_mask(frame, boxes_positive_patches_array[count])\n",
        "    svm_mask = make_mask(frame, svm_boxes_array[count], yolo=False)\n",
        "\n",
        "    # Calculating IoU\n",
        "    intersections.append(np.logical_and(yolo_mask, svm_mask))\n",
        "    unions.append(np.logical_or(yolo_mask, svm_mask))\n",
        "\n",
        "    # plt.figure(figsize=(10, 10))\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    # plt.imshow(yolo_mask)\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    # plt.imshow(svm_mask)\n",
        "    # plt.show()\n",
        "\n",
        "print(\"IoU: \" + str(np.sum(intersections) / np.sum(unions)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IoU:0.1769772970861959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "onYxT1gJ_aAk"
      },
      "source": [
        "# field names  \n",
        "fields = ['id', 'count']  \n",
        "    \n",
        "# data rows of csv file  \n",
        "rows = predictions_per_image\n",
        "\n",
        "# name of csv file  \n",
        "filename = \"group21_submission.csv\"\n",
        "    \n",
        "# writing to csv file  \n",
        "with open(filename, 'w', newline='') as csvfile:  \n",
        "    # creating a csv writer object  \n",
        "    csvwriter = csv.writer(csvfile)  \n",
        "        \n",
        "    # writing the fields  \n",
        "    csvwriter.writerow(fields)  \n",
        "        \n",
        "    # writing the data rows  \n",
        "    csvwriter.writerows(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbk8tUuk0vDP"
      },
      "source": [
        "## Todo:\n",
        "- Fix bug that happens if we extract from more than 9 images\n",
        "- Decide which feature to use between HoG, Haar, LBP, or other\n",
        "- Modify SVM code we got to make it less sus\n",
        "- Tune parameters of SVM to improve results\n",
        "- Duplicate elimination\n",
        "- Test it"
      ]
    }
  ]
}